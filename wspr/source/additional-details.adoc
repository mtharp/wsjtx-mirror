=== Main Window

// Insert Mani Screen image here

[[MAINWINDOW]]
In normal operation on a single band, your _{prog}_ screen will look something like
the illustration above.  The decoder looks for all detectable _{prog}_ signals in a
200 Hz passband and displays its results in a waterfall spectrogram, a text 
window, and a Band Map.  The spectrogram covers a narrow frequency range 
(slightly more than 200 Hz) in the vertical direction; the last three digits of
the received frequency, in Hz, are displayed on a scale at right.  Time runs 
from left to right in the spectrogram.  On a typical computer screen each 
two-minute interval is a strip about 1 cm wide.  The times of your own 
transmissions are denoted by thin green vertical lines in the spectrogram.

Each decoded _{prog}_ signal produces text showing the UTC, measured signal-to-noise
ratio in dB (in a 2500 Hz reference bandwidth), time offset DT in seconds, 
measured frequency in MHz, drift rate in Hz/minute, and the decoded message.
Time offsets greater than about ±2 seconds indicate a significant timing error
at transmitter or receiver, or possibly both.  For best performance your computer
clock should be kept accurate to within ±1 second.  Apparent frequency drifts
greater than ±1 Hz per minute can usually be traced to the transmitter, and
should be corrected if possible.  Of course, receiver drifts could also
contribute to a measured drift — but they are easily recognized because nearly
all signals will appear to drift by the same amount.


Color coding is used in the Band Map to indicate elapsed time since a station
was decoded.  Calls in red have been seen within 15 minutes of the last line of
decoded text; yellow callsigns are 15-30 minutes old, light gray 30-45 minutes,
and darker gray 45-60 minutes.  Callsigns more than an hour older than the most
recent one are deleted from the Band Map.

